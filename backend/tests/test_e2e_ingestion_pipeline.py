"""
End-to-End Tests for Complete Ingestion Pipeline

Tests the full flow: API Request → Celery Worker → SEC API → Database → API Response

These tests verify:
1. API endpoint accepts ingestion request
2. Celery worker processes the task
3. SEC EDGAR API is called correctly
4. Data is persisted to database
5. Data can be retrieved via API

This is a complete integration test spanning all system components.
"""

import pytest
import time
from unittest.mock import patch
from fastapi.testclient import TestClient


# =============================================================================
# E2E Test: Complete Ingestion Flow
# =============================================================================

@pytest.mark.e2e
@pytest.mark.db
@pytest.mark.api
@pytest.mark.celery
@pytest.mark.slow
class TestEndToEndIngestionPipeline:
    """
    Complete end-to-end test of the ingestion pipeline.
    
    Flow:
    1. Client calls POST /api/v1/company/{ticker}/ingest
    2. API enqueues Celery task
    3. Worker fetches data from SEC EDGAR
    4. Worker stores data in PostgreSQL
    5. Client calls GET /api/v1/company/{ticker}
    6. API returns ingested data from database
    """
    
    def test_complete_ingestion_flow(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client,
        mock_sec_company_facts
    ):
        """
        Test complete flow from API request to data retrieval.
        
        Steps:
        1. Verify company doesn't exist
        2. Trigger ingestion via API
        3. Wait for worker to process
        4. Verify data in database
        5. Fetch data via API
        6. Validate response
        """
        ticker = "MSFT"
        
        # Step 1: Verify company doesn't exist initially
        response = client.get(f"/api/v1/company/{ticker}")
        assert response.status_code == 404
        assert "not found" in response.json()["detail"].lower()
        
        # Step 2: Trigger ingestion
        response = client.post(f"/api/v1/company/{ticker}/ingest")
        assert response.status_code == 200
        assert response.json()["status"] == "queued"
        assert ticker in response.json()["detail"]
        
        # Step 3: Worker processes task (using eager mode, happens immediately)
        # In production, you'd need to wait or poll
        
        # Step 4: Verify data was written to database
        from app.db.models import Company, Filing
        
        company = db_session.query(Company).filter_by(ticker=ticker).first()
        assert company is not None, "Company was not created in database"
        assert company.name == "MICROSOFT CORPORATION"
        assert company.cik == "0000789019"
        
        filings = db_session.query(Filing).filter_by(cik=company.cik).all()
        assert len(filings) > 0, "No filings were created"
        
        # Step 5: Fetch data via API
        response = client.get(f"/api/v1/company/{ticker}")
        assert response.status_code == 200
        
        # Step 6: Validate API response contains ingested data
        data = response.json()
        assert "company" in data
        assert data["company"]["ticker"] == ticker
        assert data["company"]["name"] == "MICROSOFT CORPORATION"
        assert data["company"]["cik"] == "0000789019"
    
    def test_ingestion_idempotency(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test that multiple ingestion requests for same ticker are idempotent.
        
        Verifies:
        - Duplicate data isn't created
        - Database constraints are respected
        - Subsequent ingestions update existing data
        """
        ticker = "MSFT"
        
        # First ingestion
        response1 = client.post(f"/api/v1/company/{ticker}/ingest")
        assert response1.status_code == 200
        
        from app.db.models import Company
        companies_after_first = db_session.query(Company).filter_by(ticker=ticker).count()
        
        # Second ingestion (should not create duplicate)
        response2 = client.post(f"/api/v1/company/{ticker}/ingest")
        assert response2.status_code == 200
        
        companies_after_second = db_session.query(Company).filter_by(ticker=ticker).count()
        
        # Should still have only one company record
        assert companies_after_first == companies_after_second == 1
    
    def test_parallel_ingestions(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test parallel ingestion of multiple tickers.
        
        Verifies:
        - Multiple ingestions can run concurrently
        - No race conditions or deadlocks
        - All data is correctly persisted
        """
        tickers = ["MSFT", "AAPL", "AMZN"]
        
        # Trigger all ingestions
        responses = []
        for ticker in tickers:
            response = client.post(f"/api/v1/company/{ticker}/ingest")
            responses.append((ticker, response))
        
        # All should queue successfully
        for ticker, response in responses:
            assert response.status_code == 200
            assert response.json()["status"] == "queued"
        
        # Verify all companies were created
        from app.db.models import Company
        for ticker in tickers:
            company = db_session.query(Company).filter_by(ticker=ticker).first()
            assert company is not None, f"Company {ticker} was not created"


# =============================================================================
# E2E Test: Data Persistence Verification
# =============================================================================

@pytest.mark.e2e
@pytest.mark.db
class TestDataPersistence:
    """
    Test that ingested data persists correctly across the stack.
    """
    
    def test_financial_statements_persistence(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client,
        mock_sec_company_facts
    ):
        """
        Test that financial statement data is correctly extracted and stored.
        
        Verifies:
        - Revenue data is extracted
        - EPS data is extracted
        - Income statement records are created
        - Data matches SEC API response
        """
        ticker = "MSFT"
        
        # Trigger ingestion
        client.post(f"/api/v1/company/{ticker}/ingest")
        
        # Verify income statement data
        from app.db.models import StatementIS
        
        statements = db_session.query(StatementIS).join(
            StatementIS.filing
        ).filter_by(cik="0000789019").all()
        
        # Should have multiple years of data
        assert len(statements) > 0
        
        # Verify data values
        for stmt in statements:
            if stmt.eps_diluted is not None:
                assert stmt.eps_diluted > 0  # EPS should be positive for MSFT
    
    def test_company_metadata_persistence(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test that company metadata is correctly stored.
        
        Verifies:
        - CIK is stored with proper formatting
        - Company name is stored
        - Ticker is stored in uppercase
        """
        ticker = "msft"  # lowercase to test normalization
        
        client.post(f"/api/v1/company/{ticker}/ingest")
        
        from app.db.models import Company
        company = db_session.query(Company).filter_by(ticker=ticker.upper()).first()
        
        assert company is not None
        assert company.cik == "0000789019"  # Properly zero-padded
        assert company.ticker == "MSFT"  # Uppercase
        assert company.name == "MICROSOFT CORPORATION"
    
    def test_filing_dates_persistence(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test that filing dates are correctly parsed and stored.
        
        Verifies:
        - period_end dates are stored as date objects
        - Dates match fiscal years
        - Dates are in chronological order
        """
        from datetime import date
        
        client.post(f"/api/v1/company/MSFT/ingest")
        
        from app.db.models import Filing
        filings = db_session.query(Filing).filter_by(cik="0000789019").order_by(Filing.period_end).all()
        
        assert len(filings) > 0
        
        for filing in filings:
            assert isinstance(filing.period_end, date), "period_end should be a date object"
            assert filing.period_end.year >= 2020, "Should have recent filings"


# =============================================================================
# E2E Test: API Response Validation
# =============================================================================

@pytest.mark.e2e
@pytest.mark.api
class TestAPIResponseIntegrity:
    """
    Test that API responses correctly represent database state.
    """
    
    def test_get_company_response_structure(
        self,
        client,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test the structure of GET /api/v1/company/{ticker} response.
        
        Verifies:
        - Response contains all required fields
        - Data types are correct
        - Related data is properly joined
        """
        ticker = "MSFT"
        
        # Ingest data
        client.post(f"/api/v1/company/{ticker}/ingest")
        
        # Fetch data
        response = client.get(f"/api/v1/company/{ticker}")
        assert response.status_code == 200
        
        data = response.json()
        
        # Validate structure
        assert "company" in data
        assert "latest_is" in data
        
        # Validate company fields
        company = data["company"]
        assert "id" in company
        assert "cik" in company
        assert "ticker" in company
        assert "name" in company
        
        # Validate latest_is fields
        if data["latest_is"]:
            latest_is = data["latest_is"]
            assert "fy" in latest_is
            # Should have at least some financial metrics
            assert any(key in latest_is for key in ["revenue", "eps_diluted", "net_income"])
    
    def test_metrics_endpoint_after_ingestion(
        self,
        client,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test GET /api/v1/company/{ticker}/metrics after ingestion.
        
        Verifies:
        - Metrics endpoint returns data after ingestion
        - Calculated metrics are present
        - Historical data is available
        """
        ticker = "MSFT"
        
        client.post(f"/api/v1/company/{ticker}/ingest")
        
        response = client.get(f"/api/v1/company/{ticker}/metrics")
        
        # Metrics might return 200 with data or 404 if no metrics computed
        # This depends on whether metrics calculation is automatic
        assert response.status_code in [200, 404]
        
        if response.status_code == 200:
            data = response.json()
            assert isinstance(data, (dict, list))


# =============================================================================
# E2E Test: Error Scenarios
# =============================================================================

@pytest.mark.e2e
@pytest.mark.api
class TestIngestionErrorHandling:
    """
    Test error handling in the complete pipeline.
    """
    
    def test_invalid_ticker_ingestion(
        self,
        client,
        celery_worker,
        httpx_mock
    ):
        """
        Test ingestion of invalid/non-existent ticker.
        
        Verifies:
        - API accepts the request
        - Worker handles the error gracefully
        - No partial data is created
        """
        # Mock SEC API to return valid ticker list without our ticker
        httpx_mock.add_response(
            url="https://www.sec.gov/files/company_tickers.json",
            json={"0": {"cik_str": 789019, "ticker": "MSFT"}}
        )
        
        response = client.post("/api/v1/company/INVALID/ingest")
        
        # Request should be accepted
        assert response.status_code == 200
        
        # But data shouldn't be in database
        from app.db.models import Company
        from app.db.session import SessionLocal
        
        db = SessionLocal()
        try:
            company = db.query(Company).filter_by(ticker="INVALID").first()
            assert company is None
        finally:
            db.close()
    
    def test_sec_api_failure_handling(
        self,
        client,
        celery_worker,
        httpx_mock,
        db_session
    ):
        """
        Test handling of SEC API failures.
        
        Verifies:
        - Worker handles API errors
        - No partial data is committed
        - System remains stable
        """
        # Mock SEC API to return error
        httpx_mock.add_response(
            url="https://www.sec.gov/files/company_tickers.json",
            status_code=503
        )
        
        response = client.post("/api/v1/company/MSFT/ingest")
        assert response.status_code == 200  # Request accepted
        
        # No data should be created due to API failure
        from app.db.models import Company
        company = db_session.query(Company).filter_by(ticker="MSFT").first()
        # Depending on error handling, company might or might not exist
        # The key is that system doesn't crash


# =============================================================================
# E2E Test: Performance and Scalability
# =============================================================================

@pytest.mark.e2e
@pytest.mark.slow
class TestIngestionPerformance:
    """
    Test performance characteristics of the ingestion pipeline.
    """
    
    @pytest.mark.timeout(30)
    def test_single_ingestion_performance(
        self,
        client,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test that single ticker ingestion completes within reasonable time.
        
        Target: < 5 seconds for mocked data
        """
        import time
        
        start_time = time.time()
        
        response = client.post("/api/v1/company/MSFT/ingest")
        assert response.status_code == 200
        
        duration = time.time() - start_time
        
        # Should complete quickly with mocked data
        assert duration < 5.0, f"Ingestion took {duration:.2f}s, expected < 5s"
    
    @pytest.mark.timeout(60)
    def test_batch_ingestion_performance(
        self,
        client,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test performance of ingesting multiple tickers.
        
        Target: < 30 seconds for 10 tickers with mocked data
        """
        import time
        
        tickers = [f"TICK{i}" for i in range(10)]
        
        start_time = time.time()
        
        for ticker in tickers:
            response = client.post(f"/api/v1/company/{ticker}/ingest")
            assert response.status_code == 200
        
        duration = time.time() - start_time
        
        assert duration < 30.0, f"Batch ingestion took {duration:.2f}s, expected < 30s"


# =============================================================================
# E2E Test: Data Consistency
# =============================================================================

@pytest.mark.e2e
@pytest.mark.db
class TestDataConsistency:
    """
    Test that data remains consistent throughout the pipeline.
    """
    
    def test_data_integrity_after_ingestion(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client,
        mock_sec_company_facts
    ):
        """
        Test that ingested data matches source data.
        
        Verifies:
        - Numbers are accurately stored
        - No data corruption
        - Proper type conversions
        """
        client.post("/api/v1/company/MSFT/ingest")
        
        # Get data from API
        response = client.get("/api/v1/company/MSFT")
        api_data = response.json()
        
        # Get data from database
        from app.db.models import Company
        db_company = db_session.query(Company).filter_by(ticker="MSFT").first()
        
        # Verify consistency
        assert api_data["company"]["cik"] == db_company.cik
        assert api_data["company"]["ticker"] == db_company.ticker
        assert api_data["company"]["name"] == db_company.name
    
    def test_referential_integrity(
        self,
        client,
        db_session,
        celery_worker,
        mock_httpx_client
    ):
        """
        Test that foreign key relationships are maintained.
        
        Verifies:
        - Filings reference valid companies
        - Statements reference valid filings
        - No orphaned records
        """
        client.post("/api/v1/company/MSFT/ingest")
        
        from app.db.models import Company, Filing, StatementIS
        
        company = db_session.query(Company).filter_by(ticker="MSFT").first()
        filings = db_session.query(Filing).filter_by(cik=company.cik).all()
        
        # All filings should reference existing company
        for filing in filings:
            assert filing.cik == company.cik
        
        # All statements should reference existing filings
        statements = db_session.query(StatementIS).all()
        for stmt in statements:
            assert stmt.filing_id in [f.id for f in filings]
